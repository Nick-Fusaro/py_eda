{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrangleutils as wr\n",
    "import glob as glob\n",
    "import os\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to scored data sets\n",
    "path=\"\\\\folder\"\n",
    "#Define the name of your current scored set \n",
    "##where '.' is the working directory (i.e. the directory you launched the notebook from)\n",
    "#Creates a list of all .csvs in  that file source\n",
    "csv_list=glob.glob(os.path.realpath('.')+ path +'\\\\*.csv')\n",
    "df=pd.concat(list(map(pd.read_csv, csv_list)))\n",
    "#Keep a pristine copy \n",
    "df_copy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the data types in this data set. Creates a meta data data frame\n",
    "meta_data=pd.DataFrame(df.dtypes)\n",
    "meta_data['name']=meta_data.index\n",
    "#meta_data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record the original dimensions of the dataset before messing around with it \n",
    "original_dim=df.shape\n",
    "print(\"There are \" + str(original_dim[1]) + \" columns\" + \" and \"  + str(original_dim[0])+ \" rows in the original data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of categorical variables\n",
    "cat_list=list(df.select_dtypes(include=['object']))\n",
    "#Delete any categorical vars from the list you\n",
    "#know you arent interested in\n",
    "#del cat_list[('variable')]\n",
    "\n",
    "#Create a list of floats\n",
    "float_list= list(df.select_dtypes(include=['float']))\n",
    "#Create a list of ints\n",
    "int_list= list(df.select_dtypes(include=['int64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks wheher any float columns have a max of 0 and are useless\n",
    "max_zero=pd.DataFrame(df[float_list].columns[df[float_list].aggregate(max)==0])\n",
    "max_zero_list=list(df[float_list].columns[df[float_list].aggregate(max)==0])\n",
    "print(\"Warning - the following numeric columns have a maximum of 0:\")\n",
    "max_zero\n",
    "\n",
    "#Note - this will need to be deprecated and replaced with\n",
    "#a function that simply drops all constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the max zero list\n",
    "df.drop(max_zero_list, axis=1, inplace=True)\n",
    "#Drops the max zero list fom the float list for later analysis \n",
    "float_list =  [x for x in float_list if x not in max_zero_list]\n",
    "#Find the shape of the new data frame with the max 0 columns dropped \n",
    "new_dim=df.shape\n",
    "print( \"There were \"  + str(original_dim[1]) + \" columns and now are \" + str(new_dim[1]) + \" columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the max zero list\n",
    "df.drop(max_zero_list, axis=1, inplace=True)\n",
    "#Drops the max zero list fom the float list for later analysis \n",
    "float_list =  [x for x in float_list if x not in max_zero_list]\n",
    "#Find the shape of the new data frame with the max 0 columns dropped \n",
    "new_dim=df.shape\n",
    "print( \"There were \"  + str(original_dim[1]) + \" columns and now are \" + str(new_dim[1]) + \" columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspects some of the integer columns that maybe should be categorical (dummies)\n",
    "#max_one=pd.DataFrame(df[int_list].columns[df[int_list].aggregate(max)==1])\n",
    "#max_one_list=df[int_list].columns[df[int_list].aggregate(max)==1]\n",
    "#print(\"Warning - the following int columns might be better suited as categorical objects\")\n",
    "#df[max_one_list].describe()\n",
    "#DEPRECATED for more inclusive function :\n",
    "\n",
    "def find_non_cata_numeric(df,i):\n",
    "    non_cat_list = df.select_dtypes(exclude=['object'])\n",
    "    max_one = pd.DataFrame(df[df.loc[:, df.nunique(axis=0) < i].columns])\n",
    "    max_one_list = df.loc[:, df.nunique(axis=0) < i].columns\n",
    "    print(\"Warning - the following numerical columns might be better suited as categorical objects\")\n",
    "    print(pd.DataFrame(df[max_one_list].describe()))\n",
    "    return\n",
    "find_non_cata_numeric(df, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to do frequency anlysis\n",
    "def freq_iter(df, cols):\n",
    "    dflist = []\n",
    "    for i in cols:\n",
    "        series = df[i]\n",
    "        #Count the discrete values\n",
    "        type_count = [[k,v] for k,v in Counter(series).items()]\n",
    "        # Find the count of all discrete elements \n",
    "        j=pd.DataFrame.from_records(type_count, \n",
    "                                    index=range(len(type_count)), \n",
    "                                    columns=['value','frequency'])\n",
    "        j['percent'] = j.frequency/j.frequency.sum() \n",
    "        j['cumulative_frequency'] = j.frequency.cumsum()\n",
    "        j['cumulative_percent'] = j.cumulative_frequency/j.frequency.sum()\n",
    "        j.columns = [series.name + '_' + i for i in j.columns]\n",
    "        dflist.append(j)\n",
    "    return dflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with the frequency analysis function and displays the contents \n",
    "freq_iter_list = freq_iter(df, cat_list)\n",
    "for p in freq_iter_list: \n",
    "     display (p.iloc[:,[0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the variance if all the float columns\n",
    "ratio_of_std_to_mean=pd.DataFrame(df[float_list].std()) /pd.DataFrame(df[float_list].mean())\n",
    "#####\n",
    "#### Keep working on this part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates % missing by columns\n",
    "missing=pd.DataFrame(100* df.isna().sum() /len(df))\n",
    "missing['pct_missing']=missing[0]\n",
    "#Optional missing threshold\n",
    "threshold=20\n",
    "missing_above_threshold= missing[missing['pct_missing']>threshold]\n",
    "missing_above_threshold=pd.DataFrame(missing_above_threshold.iloc[:,1])\n",
    "\n",
    "#Shows all missing regardles sof threshold \n",
    "missing_above_zero= missing[missing['pct_missing']>0]\n",
    "missing_above_zero=pd.DataFrame(missing_above_zero.iloc[:,1])\n",
    "print(\"The following fields have missing values:\")\n",
    "missing_above_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Warning the following columns are missing more than 20 % of their values. Suggest either you bin, drop, or impute based on domain knowledge :\")\n",
    "missing_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This list has to be compiled manually, as far as I can tell.\n",
    "#I think that it is more effeicient to create a list of vars that should NOT\n",
    "#be coalesced to 0, then subtract that out to create the coalesce 0 list of fields\n",
    "\n",
    "bin_list = ['var1', 'var2', 'vark'] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the values of the missing above 0 dataframe to a list \n",
    "missing_above_zero_list = missing_above_zero.index.tolist()\n",
    "#Creates a list of variables (that arent in the binned list)\n",
    "#That have missing values that you want to replace with 0 \n",
    "zero_replace_list= [x for x in missing_above_zero_list if x not in bin_list]\n",
    "\n",
    "#Alters the base data frame by replacing the NaN with 0 in the specified columns above\n",
    "df[zero_replace_list]=df[zero_replace_list].fillna(0)\n",
    "print(\"Warning: the NaN with 0 replacement has been applied to the base dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically converts variables in the bin list to bins\n",
    "colname = lambda col, suffix: '{}_{}'.format(suffix, col)\n",
    "def add_quantiles(data, columns, suffix, quantiles=4, labels=None):\n",
    "        \"\"\" For each column name in columns, create a new categorical column with\n",
    "            the same name as colum, with the suffix specified added, that\n",
    "            specifies the quantile of the row in the original column using\n",
    "            pandas.qcut().\n",
    "\n",
    "            Input\n",
    "            _____\n",
    "            data:\n",
    "                pandas dataframe\n",
    "            columns:\n",
    "                list of column names in `data` for which this function is to create\n",
    "                quantiles\n",
    "            suffix:\n",
    "                string suffix for new column names ({`suffix`}_{collumn_name})\n",
    "            labels:\n",
    "                list of labels for each quantile (should have length equal to `quantiles`)\n",
    "\n",
    "            Output\n",
    "            ______\n",
    "            pandas dataframe containing original columns, plus new columns with quantile\n",
    "            information for specified columns.\n",
    "        \"\"\"\n",
    "        d = data.copy()\n",
    "        quantile_labels = labels or [\n",
    "            '{:.0f}%'.format(i*100/quantiles) for i in range(1, quantiles+1)\n",
    "        ]\n",
    "        for column in columns:\n",
    "            d[colname(column, suffix)] = pd.qcut(d[column], quantiles, labels=quantile_labels)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data framed of just binned columns\n",
    "bin_df=add_quantiles(df[bin_list], df[bin_list], 'q', quantiles=4, labels=None)\n",
    "bin_df=bin_df.drop(bin_df[bin_list], axis=1)\n",
    "#Converts the binned data frame to a data frame of dummy variables\n",
    "bin_dummies_df=pd.get_dummies(bin_df, dummy_na=True)\n",
    "#Appends the dummy variables back to original datset \n",
    "df=pd.concat([df,bin_dummies_df], axis=1)\n",
    "#delete the bin_df for memory management\n",
    "del bin_df\n",
    "#Drop the original untransformed dummy quantized fields from the main data set \n",
    "df=df.drop(df[bin_list], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts thelist of natural categorical variables to dummies\n",
    "cat_dummies_df=pd.get_dummies(df[cat_list], dummy_na=True)\n",
    "#Appends the dummy variables back to original datset \n",
    "df=pd.concat([df,cat_dummies_df], axis=1)\n",
    "#delete the bin_df for memory management\n",
    "del cat_dummies_df\n",
    "#Drop the original untransformed dummy quantized fields from the main data set \n",
    "df=df.drop(df[cat_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the changes in columns after this step \n",
    "new_dim3=df.shape\n",
    "print( \"There were \"  + str(original_dim[1]) + \" columns to start\")\n",
    "print(\"There were \"  + str(new_dim[1]) + \" columns after the Max 0 variables were dropped\")\n",
    "print(\"There were \" +str(new_dim2[.31]) + \" columns after the fields with high missing values were converted to bins, then dummies\")      \n",
    "print(\"There are \" +str(new_dim3[1]) + \" columns after original categorical fields dummies\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates % missing by columns\n",
    "missing=pd.DataFrame(100* df.isna().sum() /len(df))\n",
    "missing['pct_missing']=missing[0]\n",
    "#Shows all missing\n",
    "missing_above_zero= missing[missing['pct_missing']>0]\n",
    "missing_above_zero=pd.DataFrame(missing_above_zero.iloc[:,1])\n",
    "print(\"The following fields have missing values:\")\n",
    "missing_above_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an updated list of floats to be scaled and standardized\n",
    "new_float_list= list(df.select_dtypes(include=['float']))\n",
    "#Center/standardize/scale the continuous variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "# Subset of vars to to scale with the new float list \n",
    "to_scale_df= df[new_float_list]\n",
    "# Apply the scaler to the subset\n",
    "scaled_df = pd.DataFrame(ss.fit_transform(to_scale_df), columns =to_scale_df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
